%%%% ijcai18.tex

\typeout{IJCAI-18 Instructions for Authors}

% These are the instructions for authors for IJCAI-18.
% They are the same as the ones for IJCAI-11 with superficical wording
%   changes only.

\documentclass{article}
\pdfpagewidth=8.5in
\pdfpageheight=11in
% The file ijcai18.sty is the style file for IJCAI-18 (same as ijcai08.sty).
\usepackage{ijcai18}

% Use the postscript times font!
\usepackage{mathptmx}
\usepackage{times}
\usepackage{xcolor}
\usepackage{soul}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algpseudocode}


% the following package is optional:
%\usepackage{latexsym} 

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.

\title{Designing an Artificial Intelligence Poker Agent with Q-Learning}


% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)

\author{
Arielyte Tsen Chung Ming, Devarajan Preethi, James Pang Mun Wai, Lee Yi Wei Joel, Yip Seng Yuen
\\ 
National University of Singapore\\
CS3243 AY18/19 Semester 2 Team 07\\
%
\{chungming.tsen, preethi\_devarajan, jamespang, lywjoel, yip\}@u.nus.edu
}
% If your authors do not fit in the default space, you can increase it 
% by uncommenting the following (adjust the "2.5in" size to make it fit
% properly)
% \setlength\titlebox{2.5in}

\begin{document}

\maketitle

\section{Introduction}

The game of poker has long been a field of interest for artificial intelligence (AI) researchers. There are many challenges with developing an AI poker agent capable of competing with humans, since poker is a complex game that forces players to make decisions with incomplete and imperfect information. AI poker agents are hence faced with the task of having to make the best decision at every street under such informational constraints. Additionally, conventional poker matches impose a time limit on players to make a decision, adding another constraint for AI poker agents to work with.

This paper outlines our design of an AI poker agent for Heads Up Limit Texas Hold’em, which aims to respond competently and accurately with limited information, within a limited amount of time. In tackling the issues above, we employed a three-pronged approach:

\begin{itemize}
  \item Using Q-Learning, a reinforcement learning algorithm, to train our agent to play with the best possible strategy. (Section 3.1)
  \item Reducing the overall state size, by grouping similar states together. (Section 3.1, Clustering)
  \item Precomputing the estimated strength of hand ranks, storing them in look-up tables to allow for retrieval in linear time. (Section 3.3)
 \end{itemize}

The following two sections explain our agent’s strategy within preflop and postflop streets.

\section{Preflop Street}

In order to decide what action the agent would take based only on the limited amount of information it has during the preflop street, we introduce the use of hand strength ($HS$), which is an estimate determined by the probability of winning with the two hole cards currently in the agent’s hand.
To estimate $HS$, we perform a Monte Carlo simulation of 10,000 games for each possible starting hand. The total number of possible starting hands is 91, which is obtained by summing up the total number of possible pocket pairs and the total number of possible hands disregarding the suits. For each game, we first pick the two starting cards for the agent, followed by randomly picking two starting cards for the opponent and the five community cards. Both players’ hands are then revealed and the game result is determined as a win or a loss for the agent.

We then calculate $HS$ using the formula:
\begin{displaymath}
 HS = Pr(Win)= \frac{\# \text{ } of \text{ } wins}{Total \text{ }games \text{ } played}
\end{displaymath}
where the total games played is fixed at 10,000. The results of the Monte Carlo simulation are seen in Figure \ref{fig:1}.
\begin{figure}
  \includegraphics[width=\linewidth]{WinningProb.PNG}
  \caption{Estimated winning probabilities with starting hands}
  \label{fig:1}
\end{figure}

At every preflop street, the agent chooses an action [fold, call, raise] to take based on the $HS$ of the starting hand:
\begin{displaymath}
  \left.
  \begin{array}{l}
    raise \text{: } HS > k \\
    \\
    call \text{: } HS > j \\
    \\
    fold \text{: } HS \leq j
  \end{array}
  \right\rbrace \quad \text{where } 0 < j < k \leq 1
\end{displaymath}
where $j$ and $k$ are pre-determined thresholds set for the agent.

\section{Postflop Streets}

The postflop streets consist of the Flop, Turn and River streets. In contrast to the preflop street, at least three community cards are revealed during the postflop streets, giving us more information to work with. Here, reinforcement learning is employed. Specifically, we use a modified version of the Q-Learning algorithm to train our agent.

\subsection{Q-Learning}

As per the Q-Learning algorithm, we maintain a table of states and actions as an input to the agent. Each state has a Q-Value for each action (Fold, Call or Raise), signifying the expected reward for the action. An illustration of the Q-Learning table can be seen in Table 1.

\begin{table}[h!]
  \begin{center}
	\begin{tabular}{ c|c|c|c }
	\hline
	\multirow{2}{*}{\textbf{State}}&\multicolumn{3}{|c}{\textbf{Action}}\\
	\cline{2-4}
	& $fold$ & $call$ & $raise$\\ 
	\hline
	1\\
	2\\ 
	3\\ 
	$\cdots$\\
	S\\
	\hline
	\end{tabular}
	\caption{Q-Learning Table}
    \label{tab:table1}
  \end{center}
\end{table}
The reward function is determined based on the pot size $P$ - the total amount that has been bet by both players - at the end of the game, as seen in the formula below:
\begin{displaymath}
  R_t=
  \left\lbrace
  \begin{array}{l}
    +\frac{P}{2}\quad\text{if win,} \\
    \\
    -\frac{P}{2}\quad\text{otherwise}
  \end{array}
  \right.
\end{displaymath}

After each training round, the Q-Value of each state is obtained by summing up the previous Q-Value for that state and the reward function for that round. More formally,
\begin{displaymath}
Q\left(s',a'\right)=(1-l)Q\left(s,a\right)+l\lambda^{t}R\left(s,a\right)
\end{displaymath}
\noindent where

\begin{description}[style=multiline,leftmargin=10mm]
\item [\emph{t}]refers to the number of turns between the current move and the terminal node.
\item [\emph{l}]refers to the learning factor.
\item [\emph{R}]refers to the reward for that round.
\item [\emph{$\lambda$}]refers to the discount factor.
\item [\emph{s}]refers to the state of the agent.
\item [\emph{a}]refers to the action taken by the agent.
\item [\emph{P}]refers to the pot size.
\end{description}

\subsubsection{Experience Replay}

Additionally, we apply the technique of experience replay to achieve more efficient use of previous experiences. We store the agent's experiences based on the formula
\begin{displaymath}
e_t=\left(s_t,a_t,R_t,s_{t+1}\right)
\end{displaymath}
The agent stores the data it discovers for each round in a table, and reinforcement learning takes place based on random sampling from the table. This reduces the amount of experience required to learn, replacing it with more computation and memory which are cheaper resources than the agent's continuous interactions with the environment~\cite{sqas:replay}.

\subsubsection{$\epsilon$-Greedy Algorithm}

At every turn, with a probability of $\epsilon$, a random action is chosen to be performed, otherwise an action with the best expected reward (highest Q-Value) is chosen.

The value of $\epsilon$ will slowly decrease as the agent acquires more training. A relatively large initial value ensures that all possible paths will be explored, to avoid the scenario where the agent settles into a sub-optimal pattern of calling the same action repeatedly. At every street, the state of the agent is determined by the following attributes:
\begin{displaymath}
State_i = \{ EHS_i \text{, } S_i \text{, }P_i\text{, }\#OR_i\text{, }\#SR_i\text{, }OPS_i\}
\end{displaymath}
where

\begin{description}[style=multiline,leftmargin=10mm]
\item [\emph{EHS}]refers to the agent's current Expected Hand Strength. More details can be found in Section 3.2.
\item [\emph{S}]refers to the current street of the round.
\item [\emph{P}]refers to the pot size.
\item [\emph{\#OR}]refers to the number of raises the opponent has made in the round so far.
\item [\emph{\#SR}]refers to the number of raises the agent has made in the round so far.
\item [\emph{OPS}]refers to the Opponent Playing Style. More details can be found in Section 3.4.
\end{description}

\subsubsection{Clustering}

Using clustering to group information into buckets would reduce the massive state space, making it much more feasible to explore all possible states. We apply clustering to group the information for our Q-Learning algorithm as such:
\begin{itemize}
  \item $EHS$ is divided into groups with an increment of 0.01, from 0 to 1.
  \item $S$ represents the three postflop streets - Flop, Turn and River.
  \item $P$ ranges from \$0 to \$680 per game. We group it into increments of \$40, which is the 2 times the big blind amount, since every raise (big blind amount - \$20) must minimally be matched by the opponent. We get a range of 0 to 17.
  \item $\#OR$ simply ranges from 0 to 4.
  \item $\#SR$ simply ranges from 0 to 4.
  \item $OPS$ is grouped into four categories. More details can be found in Section 3.4.
\end{itemize}

These groupings help to reduce the complexity and cut down the size of the total state space to: $101 \times 3 \times 18 \times 5 \times 5 \times 4 = 545,400$ states


\subsection{Expected Hand Strength}

The Expected Hand Strength, $EHS$, is the probability that a given player's current hand wins if the game reaches a showdown. It factors in all possible combinations for the opponent's hand and the remaining unrevealed board cards, and compares each of these hands to the agent's hand to determine which hand wins. The $EHS$ of the agent's hand is calculated by finding the total number of times the agent's hand wins against the opponent's, out of all possible comparisons. More formally,
\begin{displaymath}
  EHS(h)= \frac{Ahead(h)+\frac{Tied(h)}{2}}{Ahead(h) + Tied(h) + Behind(h)}
\end{displaymath}

\noindent where the $Ahead$, $Tied$, and $Behind$ functions respectively determine the number of times the given player's hand wins, ties or loses against its opponent's. The above formula used to derive the $EHS$ was referenced from Teofilo \textit{et al.}~\shortcite{trc:hs}. 

Note that the $EHS$ may be required by the agent at any street of any round. However, the time required to accurately compute $EHS$ based on large samples may exceed our time constraint of 200 milliseconds. This issue can be addressed by using a lookup table, which is explained in the next section.

\subsection{\textbf{\textit{EHS}} Lookup Table}

We introduce a technique similar to the Average Rank Strength ($ARS$) heuristic~\cite{trc:ars} to improve the efficiency of $EHS$. In our method, three look-up tables are created - one for Flop, one for Turn and one for River. These tables consist of precomputed $EHS$ values for all possible hand scores. The score of a particular hand refers to its strength, and is computed by PyPokerEngine's built-in $HandEvaluator.eval\_hand()$ function. For example, $HandEvaluator.eval\_hand()$ returns a numeric representation of the agent's hand score given some combination of hole cards and community cards which contain a Pair, which would be lower than if there were a Three-of-a-Kind. More details can be found in the PyPokerEngine repository.

Instead of computing $EHS$ during gameplay, the agent now refers to the lookup table, to obtain its $EHS$ based on its current hand score. The use of this technique allows the computation to run in linear time, and results in an approximate thousand-fold improvement in response time, while compromising negligibly on accuracy~\cite{trc:ars}.

To enumerate the $EHS$ table for the three postflop streets, for each street we performed a Monte Carlo simulation of 2.5 million samples of the agent's hand and the revealed community cards, also calculating the agent's hand score for each sample. Additionally, for each sample, we performed another simulation of 500 samples of the opponent's hand and the unrevealed community cards (if any) to calculate the $EHS$ for the hand score. The results are displayed below:

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{l|c}
      \textbf{Street} & \textbf{Number of Distinct Scores} \\
      \hline
      Flop & 5133 \\
      Turn & 13,408 \\
      River & 17,470 \\
    \end{tabular}
    \caption{Results of $EHS$ Lookup Table Simulation}
    \label{tab:table2}
  \end{center}
\end{table}

\noindent We are aware that not all possible hand scores may appear in the tables generated during the simulation. However, our results from simulating 100,000 games reveal that approximately 99.87 percent of all hand scores can be found in each table, showing that the lookup tables remarkably maintain a high degree of reliability and accuracy. In the case where an agent looks up a hand score that is not enumerated in the lookup table, we perform a limited Monte Carlo simulation of 100 samples for that particular hand score and add it to the lookup table.

\subsection{Opponent Playing Styles}

The playing style of an opponent can be classified into four categories~\cite{rupeneite:Reinforcement}. Each style distinctly describes the opponent's frequency of play and style of betting. The four categories of playing styles are Loose/Passive, Loose/Aggressive, Tight/Passive and Tight/Aggressive. A brief description of each style is given in Table 3 below.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{|l|p{5cm}|}
    \hline
      \textbf{Playing Styles} & \textbf{Description} \\
      \hline
      Tight & Plays few hands and often folds. \\
      \hline
      Loose & Plays multiple and varied hands.  \\
      \hline
      Aggressive & Bets and raises a lot, almost always never checking or call. \\
      \hline
      Passive & Usually checks and call, unlikely to take the lead. \\
      \hline
    \end{tabular}
    \caption{Description of Playing Styles}
    \label{tab:table3}
  \end{center}
\end{table}

Aggressiveness Factor, $AF$, is the ratio of a given player's number of raises to the number of calls. More formally:
\begin{displaymath}
  AF = \frac{\text{\# }raises}{\text{\# }calls}
\end{displaymath}
\noindent Players are classified as either Aggressive or Passive based on their Aggressiveness Factor. Based on research by Rupeneite~\shortcite{rupeneite:Reinforcement}, a threshold of 1 is used:
\begin{itemize}
	\item Aggressive if $AF > 1$
	\item Passive if $AF \leq 1$
\end{itemize}

Player Tightness, $PT$, is the ratio of a given player's number of folds to the total number of games. More formally:
\begin{displaymath}
  PT =  \frac{\text{\# }folds}{\text{\# }games} 
\end{displaymath}
\noindent Players are classified as either Loose or Tight based on their Player Tightness. Based on research by Rupeneite~\shortcite{rupeneite:Reinforcement}, a threshold of 0.28 is used:
\begin{itemize}
	\item Tight if $PT < 0.28$
	\item Loose if $PT \geq 0.28$
\end{itemize}

Later, a classification process, introduced by Dinis and Reis~\shortcite{dinis-reis:modeling}, combined these two heuristics to classify the opponent's style of play into four categories, as seen in Table 4.

\begin{table}[h!]
  \begin{center}
    \begin{tabular}{p{1.5cm}|p{2cm}|p{2cm}}
      \textbf{} & \textbf{AF $\leq$ 1} & \textbf{AF $>$ 1} \\
      \hline
      \textbf{PT $\geq$ 0.28} & Loose \newline Passive & Loose \newline Aggressive \\
      \hline
      \textbf{PT $<$ 0.28} & Tight \newline Passive & Tight \newline Aggressive \\
    \end{tabular}
    \caption{Style of Play Classification}
    \label{tab:table4}
  \end{center}
\end{table}

\section{Analysis}

After several rounds of preliminary testing, we realized that the state space was still too large for the agent to be meaningfully trained with our resources. The following analysis was conducted with a limited set of features: Street, EHS, Pot, Action, which allowed us to generate tangible results within several hours of training.

\subsection{Training and Results}

Our poker agent was training with static agents, as detailed in Table 5.

\begin{table}[H]
  \begin{center}
    \begin{tabular}{|l|p{5cm}|}
    \hline
      \textbf{Agent} & \textbf{Description} \\
      \hline
      RandomPlayer & Chooses an action randomly. \\
      \hline
      RaisePlayer & Always raises.  \\
      \hline
      HonestPlayer & Raises on a good hand, calls on a decent hand, folds on a bad hand. \\
      \hline
    \end{tabular}
    \caption{Static Agents used for Training}
    \label{tab:table5}
  \end{center}
\end{table}

Early iterations of our agent performed rather well against Random and Raise but lost to HonestPlayer by a small margin.
The average winnings per round were determined to be:

\begin{itemize}
	\item Against RandomPlayer: 2.107 big blinds per round
	\item Against RaisePlayer: 0.4758 big blinds per round
	\item Against HonestPlayer: -0.1421 big blinds per round
\end{itemize}

Later iterations of our agent performed better; although our agent was still losing to HonestPlayer, it lost by an even smaller margin of 0.11 big blinds per round.

\subsection{Limitations}

\subsubsection{Loss of Information from Clustering of \textbf{\textit{EHS}}}

Clustering of the $EHS$ in by divisions of 0.01 was used with the intent of reducing the size of the state space. This results in different combinations of cards with similar $EHS$ sharing the same state bucket. However, the action that the agent takes may also be dependent on its current combination of cards. Hence, information is lost from when we differentiate different combinations of cards merely by their $EHS$. Further work can be done to improve the decision process, such as by reducing the size of the buckets, or further classifying card combinations through the use of other heuristics such as the rank of the hand (High Card, Pair, Straight, Flush, etc.) or Hand Potential~\cite{trc:hs}.

\subsubsection{Non-Convergent Q-Values}

The use of the $\epsilon$-Greedy algorithm determines that every turn, there is a probability of $\epsilon$ that a random action is chosen, else the action with the highest Q-Value (best expected reward) is chosen. Ideally, reducing the value of $\epsilon$ after every game would cause the Q-Values of each state to converge, which would allow the agent to more reliably choose the best action at every state. Due to the time constraints of the project, we did not manage to derive a reliable formula for varying the value of $\epsilon$. Our results show that our Q-Values did not converge after approximately 700,000 games with $\epsilon$ = 0.15 and $l$ = 0.02. However, this could also be due to the fact that we did not have a large enough number of rounds for the Q-Values to converge, or that our clustering method needs to be improved to more accurately group the Q-Values.

\subsubsection{Lack of Diversity in Training Opponents}

The selection of opponents is an important factor in determining the type of agent we produce. For instance, training against an opponent who randomly selects actions will result in a greater probability of our agent selecting calls and raises, since the opponent has a substantial probability to fold throughout the round. Hence, training against opponents with different playstyles will produce a different set of biases for each opponent. We conclude that games against these static opponents would only be useful for initial training, and further generations should be trained by playing against itself. Further research can consider exploring a greater variety of opponent profiles to train the agent against.

\section{Conclusions}

There are still improvements that can be made to our agent for it to more effectively excel at Poker. While our research has resulted in a successful attempt at designing an agent with vast improvements to its decision time, better opponent modelling techniques and training methods can improve the overall competency of our agent.

\iffalse
\section{Hand Potential}

Hand Potential is an extension of the Hand Strength algorithm, whereby instead of merely considering the available cards in hand and in the community pool, it also considers the as yet unrevealed community cards and possible opponent hands that might improve. In the River street, this algorithm will output the same result as Hand Strength.

Hand Potential can be split into Positive Potential, $PPot$ and Negative Potential $NPot$, which are defined as such:
\begin{description}[style=multiline,leftmargin=10mm]
\item [\emph{PPot}]All scenarios where the agent is behind but eventually wins.
\item [\emph{NPot}]All scenarios where the agent is ahead but eventually loses.
\end{description}

The algorithm for Hand Potential is described in detail via pseudo-code in Algorithm 1.

  \begin{algorithm}
   \caption{Hand Potential}
    \begin{algorithmic}[1]
      \Function{HandPotential}{$agCard$, $commCard$}              
        \State Let $HP[3][3]$ and $HPTotal[3]$ be new arrays
        \State $agRank \leftarrow Rank(agCard$, $commCard)$
        \For{\textbf{each} $opCard$}
            \State $opRank \leftarrow Rank(opCard$, $commCard)$
            \If{$agRank > opRank$}
              \State $index \leftarrow ahead$
            \ElsIf{$agRank == opRank$}
              \State $index \leftarrow tied$
            \Else
              \State $index \leftarrow behind$
            \EndIf
            \State $HPTotal[index]++$
            \For{\textbf{each} $scenario$}
              \State $agBest \leftarrow Rank(agCard, scenario)$
              \State $opBest \leftarrow Rank(opCard, scenario)$
              \If{$agBest > opBest$}
                \State $HP[index][ahead]++$
              \ElsIf{$agBest == opBest$}
                \State $HP[index][tied]++$
              \Else
                \State $HP[index][behind]++$
              \EndIf
            \EndFor
        \EndFor
        \State $PPot=\frac{HP[behind][ahead] + \frac{HP[behind][tied]}{2} +
  \frac{HP[tied][ahead]}{2})}{HPTotal[behind]+\frac{HPTotal[tied]}{2}}$
		\State $NPot = \frac{HP[ahead][behind] + \frac{HP[tied][behind]}{2} + \frac{HP[ahead][tied]}{2}}{HPTotal[ahead]+\frac{HPTotal[tied]}{2}}$
        \State\Return{$PPot$, $NPot$}
      \EndFunction
    \end{algorithmic}
  \end{algorithm}

\section{Effective Hand Strength}

The probability of winning, $Pr(Win)$, can be calculated as:
\begin{displaymath}
  Pr(Win) = HS * (1-NPot) + (1-HS) * PPot
\end{displaymath}
To calculate $EHS$, we set $NPot=0$ since we want the probability of the hand being the best or improving to become the best. Thus, we eventually get the equation of $EHS$ to be:
\begin{displaymath}
  EHS = HS +(1-HS) * PPot
\end{displaymath}

\subsubsection{Counterfactual Regret Minimisation}

Counterfactual Regret Minimisation (CFR) is another reinforcement learning algorithm that we could have used while implementing our poker agent. This is an algorithm that seeks to minimise regret about its decisions at each step of a game. There are two types of regret - positive and negative regret. Negative regret refers to the regret of having taken a particular action in a particular situation. It means the agent would have done better had it not chosen this action in this situation. Positive regret is mechanism by which the agent tracks actions that resulted in a positive outcome. Unlike Q-Learning, which remembers the reward received for every action and chooses the action with the highest reward, CFR remembers the regret for every action and favours the action it regretted not having taken previously. However, CFR assumes that a terminal state is eventually reached and performs updates only after this occurs, which is not a requirement for traditional algorithms like Q-Learning.

\subsection{Word Processing Software}

As detailed below, IJCAI has prepared and made available a set of
\LaTeX{} macros and a Microsoft Word template for use in formatting
your paper. If you are using some other word processing software (such
as WordPerfect, etc.), please follow the format instructions given
below and ensure that your final paper looks as much like this sample
as possible.

{\bf Note that I did not edit the word document, and it still contains the original IJCAI formatting instructions. Please ignore those!}

\section{Style and Format}

\LaTeX{} and Word style files that implement these instructions
can be retrieved electronically. (See Appendix~\ref{stylefiles} for
instructions on how to obtain these files.)

\subsection{Layout}

Print manuscripts two columns to a page, in the manner in which these
instructions are printed. The exact dimensions for pages are:
\begin{itemize}
\item left and right margins: .75$''$
\item column width: 3.375$''$
\item gap between columns: .25$''$
\item top margin---first page: 1.375$''$
\item top margin---other pages: .75$''$
\item bottom margin: 1.25$''$
\item column height---first page: 6.625$''$
\item column height---other pages: 9$''$
\end{itemize}

All measurements assume an 8-1/2$''$ $\times$ 11$''$ page size. For
A4-size paper, use the given top and left margins, column width,
height, and gap, and modify the bottom and right margins as necessary.

\subsection{Format of Electronic Manuscript}

For the production of the electronic manuscript, you must use Adobe's
{\em Portable Document Format} (PDF). A PDF file can be generated, for
instance, on Unix systems using {\tt ps2pdf} or on Windows systems
using Adobe's Distiller. There is also a website with free software
and conversion services: {\tt http://www.ps2pdf.com/}. For reasons of
uniformity, use of Adobe's {\em Times Roman} font is strongly suggested. In
\LaTeX2e{}, this is accomplished by putting
\begin{quote} 
\mbox{\tt $\backslash$usepackage\{times\}}
\end{quote}
in the preamble.\footnote{You may want also to use the package {\tt
latexsym}, which defines all symbols known from the old \LaTeX{}
version.}
  
Additionally, it is of utmost importance to specify the American {\bf
letter} format (corresponding to 8-1/2$''$ $\times$ 11$''$) when
formatting the paper. When working with {\tt dvips}, for instance, one
should specify {\tt -t letter}.

\subsection{Title and Author Information}

Center the title on the entire width of the page in a 14-point bold
font. The title should be capitalized using Title Case. Below it, center author name(s) in a 12-point bold font. On the following line(s) place the affiliations, each affiliation on its own line using a 12-point regular font. Matching between authors and affiliations can be done using superindices. Additionally, a comma-separated email addresses list using a 12-point regular font is also allowed. Credit to a
sponsoring agency can appear on the first page as a footnote.

\subsection{Text}

The main body of the text immediately follows the abstract. Use
10-point type in a clear, readable font with 1-point leading (10 on
11).

Indent when starting a new paragraph, except after major headings.

\subsection{Headings and Sections}

When necessary, headings should be used to separate major sections of
your paper. (These instructions use many headings to demonstrate their
appearance; your paper should have fewer headings.). All headings should be capitalized using Title Case.

\subsubsection{Section Headings}

Print section headings in 12-point bold type in the style shown in
these instructions. Leave a blank space of approximately 10 points
above and 4 points below section headings.  Number sections with
arabic numerals.

\subsubsection{Subsection Headings}

Print subsection headings in 11-point bold type. Leave a blank space
of approximately 8 points above and 3 points below subsection
headings. Number subsections with the section number and the
subsection number (in arabic numerals) separated by a
period.

\subsubsection{Subsubsection Headings}

Print subsubsection headings in 10-point bold type. Leave a blank
space of approximately 6 points above subsubsection headings. Do not
number subsubsections.

\subsubsection{Special Sections}

You may include an unnumbered acknowledgments section, including
acknowledgments of help from colleagues.

Any appendices directly follow the text and look like sections, except
that they are numbered with capital letters instead of arabic
numerals.

The references section is headed ``References,'' printed in the same
style as a section heading but without a number~\cite{russell-norvig:Modern}. A sample list of
references is given at the end of these instructions~\cite{rupeneite:Reinforcement}. Use a consistent
format for references, such as that provided by Bib\TeX{}. The reference
list should not include unpublished work~\cite{trc:hs}.

\subsection{Citations}

Citations within the text should include the author's last name and
the year of publication, for example~\cite{trc:ars}.  Append
lowercase letters to the year in cases of ambiguity.  Treat multiple
authors as in the following examples:
or (for more than two authors) and (for two authors).  If the author
portion of a citation is obvious, omit it, e.g.,
Nebel.  Collapse multiple citations as
follows:
~\cite{trc:ars}
~\cite{trc:hs}
~\cite{rupeneite:Reinforcement}
~\cite{russell-norvig:Modern}
~\cite{sqas:replay}

\subsection{Footnotes}

Place footnotes at the bottom of the page in a 9-point font.  Refer to
them with superscript numbers.\footnote{This is how your footnotes
should appear.} Separate them from the text by a short
line.\footnote{Note the line separating these footnotes from the
text.} Avoid footnotes as much as possible; they interrupt the flow of
the text.

\section{Illustrations}

Place all illustrations (figures, drawings, tables, and photographs)
throughout the paper at the places where they are first discussed,
rather than at the end of the paper. If placed at the bottom or top of
a page, illustrations may run across both columns.

Illustrations must be rendered electronically or scanned and placed
directly in your document. All illustrations should be in black and
white, as color illustrations may cause problems. Line weights should
be 1/2-point or thicker. Avoid screens and superimposing type on
patterns as these effects may not reproduce well.

Number illustrations sequentially. Use references of the following
form: Figure 1, Table 2, etc. Place illustration numbers and captions
under illustrations. Leave a margin of 1/4-inch around the area
covered by the illustration and caption.  Use 9-point type for
captions, labels, and other text in illustrations.
\fi

\section*{Acknowledgments}

The preparation of this report would not have been possible without the help of Dr. Yair Zick and Arka Maity, National University of Singapore, School of Computing.

\appendix

\iffalse
\section{\LaTeX{} and Word Style Files}\label{stylefiles}

The \LaTeX{} and Word style files are available on the IJCAI--18
website, {\tt http://www.ijcai-18.org/}.
These style files implement the formatting instructions in this
document.

The \LaTeX{} files are {\tt ijcai18.sty} and {\tt ijcai18.tex}, and
the Bib\TeX{} files are {\tt named.bst} and {\tt ijcai18.bib}. The
\LaTeX{} style file is for version 2e of \LaTeX{}, and the Bib\TeX{}
style file is for version 0.99c of Bib\TeX{} ({\em not} version
0.98i). The {\tt ijcai18.sty} file is the same as the {\tt
ijcai07.sty} file used for IJCAI--07.

The Microsoft Word style file consists of a single file, {\tt
ijcai18.doc}. This template is the same as the one used for
IJCAI--07.

These Microsoft Word and \LaTeX{} files contain the source of the
present document and may serve as a formatting sample.  
\fi

%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{reportLatest}

\end{document}
